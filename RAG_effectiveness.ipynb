{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267f552e-1068-4843-bac5-c2b33ad8b035",
   "metadata": {},
   "source": [
    "Effectiveness of RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1efd68a5-14dd-4587-80f2-5188da68ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d7851f-fbc0-4b05-800a-6ee58b483a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f028f7f4-5a54-4d76-88b5-073c677467b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install llama-index-readers-file pymupdf\n",
    "#!pip3 install llama-index-vector-stores-postgres\n",
    "#!pip3 install llama-index-llms-llama-cpp\n",
    "#!pip3 install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727b4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265be3ab-6754-4424-bbf8-8f8f5922a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pypdf\n",
    "#!pip3 install transformers einops accelerate langchain bitsandbytes\n",
    "#!pip3 install sentence_transformers #Embedding\n",
    "#!pip3 install llama_index\n",
    "#!pip3 install llama-index-embeddings-langchain\n",
    "#!pip3 install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c820e57-8ebe-4dfd-b581-c8eed68fc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext  #Vector store index is for indexing the vector\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f15fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(r'/Users/joudi/Desktop/General/RUB/WISE 2425/Programming for Modern Machine Learning/Project/Content').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687a9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_prompt=\"\"\"\n",
    "#You are a Q&A assistant. Your goal is to answer questions as\n",
    "#accurately as possible based on the instructions and context provided.\n",
    "#\"\"\"\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1e1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0fd23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/joudi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use Hugging Face token\n",
    "token = \"\"\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15855dd",
   "metadata": {},
   "source": [
    "Sentence Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22697480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41f4f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Example sentence to embed\"\n",
    "embedding = embed_model.get_query_embedding(text)\n",
    "#print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806c58b",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4de10952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b3fc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/joudi/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.34 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   170.20 MiB, ( 1245.92 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2973.75 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    76.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3050.00 MiB, K (f16): 1525.00 MiB, V (f16): 1525.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 627\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.1, # let it be more focused and deterministic, avoid creativity\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5155833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install psycopg2-binary pgvector asyncpg \"sqlalchemy[asyncio]\" greenlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f8bbb",
   "metadata": {},
   "source": [
    "Create the DB and VectorStore where our documents will be stored and retrieved from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10d3f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509a4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"1234\"\n",
    "port = \"5432\"\n",
    "user = \"joudi\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a96deecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff376ce",
   "metadata": {},
   "source": [
    "We will create a function to process multiple documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "907e2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8629de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text content of a PDF file.\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f56953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_document(file_path, vector_store, embed_model):\n",
    "    loader = PyMuPDFReader()\n",
    "    documents = loader.load(file_path)\n",
    "    # Read and process document\n",
    "    # Read and sanitize text\n",
    "    document_text = read_pdf(file_path)  # Reading the PDF content\n",
    "    sanitized_text = document_text.replace('\\x00', '')  # Remove null bytes\n",
    "    \n",
    "\n",
    "    text_parser = SentenceSplitter(chunk_size=1024)\n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(sanitized_text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(text=text_chunk)\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=\"all\")\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "        nodes.append(node)\n",
    "\n",
    "    vector_store.add(nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c277140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple files\n",
    "file_paths = [\"./data/llama2.pdf\", \"./data/A Scoring System for Assessing Security and Privacy Risks of Pre Installed Applications.pdf\", \n",
    "              \"./data/Android data detection system.pdf\",\n",
    "              \"./data/Anonymous Trillemma.pdf\",\n",
    "              \"./data/Examining_the_Integrity_of_Apples_Privacy_Labels_.pdf\",\n",
    "              \"./data/Hidden Permissions on Android_ A Permission Based Android Mobile Privacy Risk Model.pdf\",\n",
    "              \"./data/6-CNS1-VisualPerception-LaurenzWiskott-LectureNotes.pdf\",\n",
    "              \"./data/Android Mobile OS Snooping By Samsung, Xiaomi, Huawei and Realme Handsets.pdf\",\n",
    "            \"./data/Apple privacy of default apps.pdf\",\n",
    "            \"./data/Cross-BorderTransfersofPersonalDataAfterSchremsII_SupplementaryMeasuresandnewStandardContractualClausesSCCs.pdf\",\n",
    "            \"./data/Examining_the_Integrity_of_Apples_Privacy_Labels_.pdf\",\n",
    "            \"./data/Exploring Considerations in the Adoption of Third-Party Services by Websites.pdf\",\n",
    "            \"./data/Hidden Permissions on Android_ A Permission Based Android Mobile Privacy Risk Model.pdf\",\n",
    "            \"./data/Runtime Permission Issues in Android Apps.pdf\"\n",
    "              ]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    process_and_store_document(file_path, vector_store, embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66fd7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In addition,\n",
      "while on Android apps may be sideloaded over adb, all of the\n",
      "handsets provided include the Google Play store and for most\n",
      "users this is the primary way to install apps. Other than on\n",
      "/e/OS, use of the Google Play store requires the user to sign\n",
      "in to a Google account and so disclose their email address\n",
      "and perhaps other personal details. We therefore also include\n",
      "opening of the handset Google Play store app and login to a\n",
      "Google account in our tests.\n",
      "With these considerations in mind, for each handset we\n",
      "carry out the following experiments:\n",
      "1) Start the handset following a factory reset (mimicking a\n",
      "user receiving a new phone), recording the network activity.\n",
      "2) Insert a SIM, recording the network activity.\n",
      "3) Following startup, leave the handset untouched for sev-\n",
      "eral days (with power cable connected) and record the network\n",
      "activity. This allows us to measure the connections made\n",
      "16There is also an important practical dimension to this assumption.\n",
      "Namely, each handset has a wide variety of settings that can be adjusted by a\n",
      "user and the settings on each handset are generally not directly comparable.\n",
      "Exploring all combinations of settings between a pair of handsets is therefore\n",
      "impractical. A further reason is that the subset of settings that a user is\n",
      "explicitly asked to select between (typically during ﬁrst startup of the handset)\n",
      "reﬂects the design choices of the handset developer, presumably arrived at\n",
      "after careful consideration and weighing of alternatives. Note that use of non-\n",
      "standard option settings may also expose the handset to ﬁngerprinting.\n",
      "when the handset is sitting idle. This test is repeated with\n",
      "the user being logged in and logged out, and with location\n",
      "enabled/disabled.\n",
      "4) Open the pre-installed Google Play app and log in to a\n",
      "user account, recording the network activity. Then log out and\n",
      "close the app store app.\n",
      "5) Open the settings app and view every option but leave\n",
      "the settings unchanged, recording the network activity. Then\n",
      "close the app.\n",
      "6) Open the settings app and enable location, then disable.\n",
      "Record the network activity.\n",
      "7) Make and receive a phone call, send and receive a text.\n",
      "Record the network activity.\n",
      "D. Additional Material: Connection Data\n",
      "The content of connections is summarised and annotated\n",
      "in\n",
      "the\n",
      "additional\n",
      "material\n",
      "available\n",
      "anonymously\n",
      "at\n",
      "https://www.dropbox.com/s/b137n94i9rpp177/additional\n",
      "material neversleepingears.pdf.\n",
      "V. RESULTS\n",
      "As already noted, Table I gives an overview of the data\n",
      "collection observed on the handsets studied. It is helpful to\n",
      "consider this in light of four basic questions: (i) who is\n",
      "collecting data, (ii) what sort of data is being collected, (iii)\n",
      "can resettable identiﬁers be relinked to the device, (iv) what\n",
      "is the potential for cross-linking of data collected by different\n",
      "parties.\n",
      "A. Who Is Collecting Data?\n",
      "1) Mobile OS Developers: We observe that Samsung, Xi-\n",
      "aomi, Realme and Huawei all collect data from user handsets,\n",
      "despite the user having opted out of data collection/teleme-\n",
      "try/analytics and making no use of services offered by these\n",
      "companies. This data is tagged with long-lived identiﬁers that\n",
      "tie it to the physical device, including across factory resets.\n",
      "In contrast, LineageOS and /e/OS were not observed to\n",
      "collect handset data. The latter is notable because a case might\n",
      "be made for the necessity of mobile OS operators collecting\n",
      "handset data in order to monitor software operation and catch\n",
      "problems early (i.e. devops). However, it is hard to justify the\n",
      "necessity of such data collection, i.e. that users should have no\n",
      "opt-out, when two mobile OSes adopt an opt-in approach. It\n",
      "is also worth noting that it can be hard to distinguish between\n",
      "diagnostics for existing software and beta testing (or A/B\n",
      "testing) for new or updated software/features. Traditionally,\n",
      "beta testing has always been opt-in. Finally, it is worth noting\n",
      "that it is hard to see why data collection for diagnostics cannot\n",
      "be carried out in a fully anonymous manner, without any use\n",
      "of long-lived identiﬁers.\n",
      "2) Pre-installed Third-Party System Apps: System apps are\n",
      "pre-installed on the /system partition of the handset disk.\n",
      "Since this partition is read-only, these apps cannot be removed.\n",
      "They are also privileged in the sense that they can be assigned\n",
      "permissions without needing user consent, be silently started,\n",
      "etc. The Settings app is, for example, a system app.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "query_str = \"where do we download android apps from?\"\n",
    "query_embedding = embed_model.get_query_embedding(query_str)\n",
    "# construct vector store query\n",
    "\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")\n",
    "\n",
    "# returns a VectorStoreQueryResult\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3fe98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e697858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff1f5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the context with prompt to LLM\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "978672a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   20047.90 ms\n",
      "llama_print_timings:      sample time =       0.71 ms /    32 runs   (    0.02 ms per token, 45261.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =  103819.84 ms /  2349 tokens (   44.20 ms per token,    22.63 tokens per second)\n",
      "llama_print_timings:        eval time =  428067.89 ms /    31 runs   (13808.64 ms per token,     0.07 tokens per second)\n",
      "llama_print_timings:       total time =  531927.01 ms /  2380 tokens\n"
     ]
    }
   ],
   "source": [
    "query_str = \"How do low-latency and low-bandwidth communication protocols, like Tor, impact the level of anonymity they provide?\"\n",
    "\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2bd4c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google Play store.\n",
      "\n",
      "Note: The answer is based on the context information provided, and it may not be the only source to download android apps.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92c2071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a0176",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82967fd4",
   "metadata": {},
   "source": [
    "Try without giving the context to LLM (directly acquiring LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "499e0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from ctransformers import AutoModelForCausalLM, AutoConfig, Config\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "    model_file=\"llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    model_type=\"llama\",\n",
    "    config=AutoConfig(\n",
    "        config=Config(\n",
    "           context_length=2048\n",
    "        )\n",
    "    ),\n",
    "    gpu_layers=50)\n",
    "\n",
    "prompt = \"where do we download android apps from?\"\n",
    "full_prompt = \"A chat between a curious user and an artificial intelligence assistant. \"\\\n",
    "              f\"The assistant gives detailed answers to the user's questions. USER: {prompt} ASSISTANT:\"\n",
    "\n",
    "response = llm(full_prompt)\n",
    "print(response)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c127fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "def get_model_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88fa507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joudi/Desktop/General/RUB/WISE 2425/Programming for Modern Machine Learning/Project/myenv/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/joudi/Desktop/General/RUB/WISE 2425/Programming for Modern Machine Learning/Project/myenv/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 29.43s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model, base_tokenizer = get_model_tokenizer(\"meta-llama/Llama-2-7b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dab43622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs(model, tokenizer, prompt, generation_config):\n",
    "    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **tokenized_input,\n",
    "        generation_config=generation_config,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    outputs = tokenizer.batch_decode(output, skip_special_tokens=True)  \n",
    "    # Batch decode is the same as decode, the only difference is that\n",
    "    # it decodes multiple token sequences at the same time\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bde5125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.5,\n",
    "    seed=1,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ca6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How do low-latency and low-bandwidth communication protocols, like Tor, impact the level of anonymity they provide?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa86164",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_outputs(\n",
    "    base_model,\n",
    "    base_tokenizer,\n",
    "    prompt,\n",
    "    generation_config\n",
    ")\n",
    "for output in outputs:\n",
    "    print(output)\n",
    "    print(\"\\n\\n------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce8efb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chat_template(prompt, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ]\n",
    "    return chat_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generate_outputs(\n",
    "    chat_model, \n",
    "    chat_tokenizer, \n",
    "    convert_to_chat_template(prompt, chat_tokenizer),\n",
    "    generation_config\n",
    ")\n",
    "for output in outputs:\n",
    "    print(output)\n",
    "    print(\"\\n\\n------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47932afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "access_token = \"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\"\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model, \n",
    "    use_auth_token=access_token\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'Hi! Tell me about yourself!',\n",
    "    do_sample=True,\n",
    ")\n",
    "print(sequences[0].get(\"generated_text\"))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
