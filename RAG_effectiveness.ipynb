{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267f552e-1068-4843-bac5-c2b33ad8b035",
   "metadata": {},
   "source": [
    "Effectiveness of RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1efd68a5-14dd-4587-80f2-5188da68ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d7851f-fbc0-4b05-800a-6ee58b483a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f028f7f4-5a54-4d76-88b5-073c677467b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install llama-index-readers-file pymupdf\n",
    "#!pip3 install llama-index-vector-stores-postgres\n",
    "#!pip3 install llama-index-llms-llama-cpp\n",
    "#!pip3 install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727b4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265be3ab-6754-4424-bbf8-8f8f5922a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pypdf\n",
    "#!pip3 install transformers einops accelerate langchain bitsandbytes\n",
    "#!pip3 install sentence_transformers #Embedding\n",
    "#!pip3 install llama_index\n",
    "#!pip3 install llama-index-embeddings-langchain\n",
    "#!pip3 install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c820e57-8ebe-4dfd-b581-c8eed68fc7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joudi/Desktop/General/RUB/WISE 2425/Programming for Modern Machine Learning/Project/myenv/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext  #Vector store index is for indexing the vector\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f15fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(r'/Users/joudi/Desktop/General/RUB/WISE 2425/Programming for Modern Machine Learning/Project/Content').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687a9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_prompt=\"\"\"\n",
    "#You are a Q&A assistant. Your goal is to answer questions as\n",
    "#accurately as possible based on the instructions and context provided.\n",
    "#\"\"\"\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1e1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a0fd23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/joudi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use Hugging Face token\n",
    "token = \"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\"\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15855dd",
   "metadata": {},
   "source": [
    "Sentence Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22697480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f4f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03405700623989105, 0.0070554292760789394, 0.014605021104216576, -0.018728939816355705, -0.024756422266364098, 8.548828191123903e-05, -0.02624456398189068, 0.03174564987421036, -0.02356886677443981, -0.011832842603325844, -0.040152426809072495, -0.05128413438796997, 0.014602968469262123, 0.004845225717872381, 0.06199422851204872, 0.033396583050489426, -0.007013244088739157, 0.056192461401224136, -0.040272362530231476, 0.03440988436341286, 0.035986579954624176, -0.027593329548835754, 0.0009162819478660822, -0.02618716098368168, -0.01858232542872429, 0.010486346669495106, -0.007246198132634163, 0.0024658306501805782, 0.00597108481451869, -0.16691185534000397, -0.03862934187054634, -0.07072882354259491, 0.02559966966509819, 0.028591390699148178, -0.006375737488269806, 0.005264699924737215, -0.06522958725690842, -0.002722624922171235, -0.03653334081172943, 0.052098751068115234, 0.011095142923295498, 0.03895411267876625, 2.7665213565342128e-05, -0.028083451092243195, -0.007340406533330679, -0.03409663587808609, -0.03230985999107361, -0.03187635913491249, -0.0068696592934429646, 0.015674922615289688, 0.05644483491778374, -0.010236512869596481, -0.008089610375463963, 0.013874450698494911, 0.009938105009496212, 0.00588076701387763, 0.0359179861843586, 0.03729166463017464, -0.03056381456553936, 0.028023429214954376, 0.01117096096277237, 0.0025816727429628372, -0.16504491865634918, 0.0065317340195178986, 0.006112015340477228, 0.0306788831949234, -0.032242752611637115, 0.01554922480136156, 0.020363319665193558, 0.08226314932107925, -0.015037047676742077, -0.033180609345436096, -0.01686478592455387, 0.05976156145334244, -0.01856490969657898, 0.05669257044792175, 0.03975875675678253, 0.01140134409070015, 0.01593518815934658, -0.019167497754096985, 0.027451999485492706, 0.03720719739794731, 0.007521406281739473, -0.025068381801247597, -0.0686398297548294, -0.039160311222076416, 0.03182906284928322, -0.03887808322906494, 0.026435570791363716, 0.022177891805768013, -0.015091911889612675, -0.0138274896889925, -0.010716292075812817, -0.005976285319775343, -0.06187085807323456, -0.0638338103890419, 0.030979923903942108, 0.005326438695192337, 0.00189424236305058, 0.5072231292724609, -0.02123142220079899, 0.033518847078084946, 0.047178931534290314, -0.09187386929988861, 0.034051258116960526, -0.04440588876605034, 0.04114706069231033, 0.003873104928061366, -0.02794218622148037, 0.006875358056277037, -0.07853933423757553, -0.007838458754122257, 0.036936819553375244, -0.0341469906270504, 0.04395352676510811, -0.022297073155641556, 0.03144864737987518, 0.031439706683158875, -0.0107429763302207, -0.023560574278235435, -0.031015202403068542, 0.01758361980319023, 0.014076581224799156, -0.020156728103756905, 0.02555466629564762, -0.02915252558887005, 0.07152455300092697, 0.0442475751042366, 0.05558640882372856, 0.012979797087609768, 0.04177803918719292, -0.03254270926117897, -0.02864363044500351, 0.04936044290661812, -0.03742760792374611, 0.042894549667835236, -0.011308779940009117, -0.00476872269064188, 0.017579732462763786, -0.044956009835004807, -0.019747335463762283, -0.014101173728704453, -0.027675781399011612, -0.06594003736972809, -0.05189094692468643, 0.08742032945156097, -0.02001340687274933, -0.01915747858583927, -0.013746119104325771, 0.015403982251882553, 0.007020284421741962, 0.007718176115304232, 0.007394940126687288, -0.002040098188444972, 0.024953989312052727, 0.07988552749156952, 0.03497261554002762, -0.015325133688747883, -0.02547648921608925, 0.021847229450941086, 0.0012689356226474047, -0.03411559388041496, -0.039666127413511276, 0.07703578472137451, 0.03287404030561447, -0.12054570764303207, -0.040074337273836136, -0.001416438608430326, 0.04954592511057854, -0.07638592272996902, 0.0493149608373642, -0.01655188947916031, -0.04363343492150307, 0.0558435320854187, 0.02587992325425148, 0.02749432437121868, -0.019503185525536537, -0.02094864845275879, 0.023464161902666092, 0.008243653923273087, 0.040980905294418335, -0.05106968805193901, -0.08404718339443207, 0.0434560663998127, -0.022925933822989464, -0.05689888447523117, -0.01544993743300438, -0.026615625247359276, 0.037408292293548584, 3.609658233472146e-05, -0.030709387734532356, 0.03933984413743019, -0.051321420818567276, 0.00526922894641757, -0.06534891575574875, 0.004324940964579582, 0.0177207812666893, -0.04995981603860855, -0.009991388767957687, -0.04750929772853851, 0.009146389551460743, 0.04078805446624756, -0.013998804613947868, 0.009283910505473614, -0.0362149253487587, -0.0108023127540946, 0.02800917997956276, -0.025194913148880005, 0.04650088772177696, -0.003887960221618414, -0.04840268939733505, -0.020823901519179344, 0.012681284919381142, 0.024855729192495346, -0.01664414070546627, 0.013576473109424114, -0.04300883784890175, 0.00774812139570713, 0.10288376361131668, 0.04534114897251129, -0.008684655651450157, -0.07575099170207977, -0.021877415478229523, -0.26232630014419556, -0.02812212146818638, 0.018206506967544556, -0.02784152515232563, 0.03834402188658714, -0.040590446442365646, 0.01911471039056778, -0.0007671694038435817, 0.012867526151239872, 0.03752666339278221, 0.06502803415060043, -0.07777626067399979, 0.039663996547460556, -0.02734597586095333, 0.012338781729340553, 0.038321055471897125, -0.008272284641861916, -0.011555816046893597, 0.050413087010383606, -0.0008522600401192904, 0.03033246472477913, -0.008308925665915012, -0.027258897200226784, -0.0222417451441288, 0.054755836725234985, -0.03764219954609871, 0.15401741862297058, 0.06350389868021011, 0.03281408175826073, -0.058491818606853485, 0.027607711032032967, 0.018732214346528053, -0.021699275821447372, -0.14447267353534698, 0.008155044168233871, -0.011607534252107143, 0.019153354689478874, 0.021122394129633904, -0.01827162504196167, -0.009224283508956432, -0.04527190700173378, 0.034631237387657166, -0.024603618308901787, -0.029329653829336166, -0.011609571054577827, -0.02900400198996067, -0.027046073228120804, -0.012660169042646885, -0.0734441876411438, 0.05811275541782379, 0.01886996254324913, -0.02659863792359829, 0.004703587386757135, -0.00338722113519907, -0.021898699924349785, 0.02028493396937847, -0.04599316790699959, 0.004739630501717329, -0.02760815992951393, 0.00017546828894410282, 0.007054637651890516, -0.028116170316934586, 0.010592088103294373, -0.04804420471191406, 0.012646854855120182, 0.013867572881281376, 0.046356070786714554, -0.025611044839024544, -0.00854664295911789, 0.01536004338413477, -0.057695016264915466, 0.08636835962533951, -0.05144108831882477, -0.016180824488401413, 0.03400757163763046, 0.014244291931390762, 0.008618797175586224, -0.014845490455627441, -0.0018464828608557582, -0.014675605110824108, 0.043833233416080475, 0.025914328172802925, 0.04842459782958031, 0.0045777433551847935, 0.046239543706178665, 0.057188648730516434, 0.04522453248500824, -0.01313857827335596, 0.02183753252029419, 0.015796372666954994, -0.021377289667725563, -0.03991609066724777, -0.00039829532033763826, 0.01693017967045307, 0.012976933270692825, -0.028594527393579483, -0.2974175214767456, 0.04040981084108353, 0.035439953207969666, 0.05579760670661926, -0.0032818277832120657, 0.01617177203297615, -0.04318300634622574, -0.01712019555270672, -0.02636241354048252, 0.03377501666545868, 0.015738945454359055, 0.018917378038167953, 0.05371297895908356, -0.002706012222915888, -0.007026535924524069, 0.04061654210090637, 0.03329656645655632, -0.025217553600668907, 0.023659106343984604, -0.03264513984322548, 0.026936182752251625, 0.009785779751837254, 0.1734420210123062, -0.0364963598549366, 0.03630729019641876, -0.017421983182430267, -0.029988467693328857, -0.027913881465792656, 0.008663908578455448, 0.018626680597662926, 0.026127131655812263, 0.03602299466729164, 0.08446510136127472, 0.02561221644282341, 0.015284040942788124, -0.012141070328652859, 0.007690349128097296, 0.03015955537557602, 0.059335190802812576, 0.0005727784591726959, -0.013923229649662971, -0.0049705165438354015, -0.05289372429251671, 0.004892013967037201, 0.00561759527772665, 0.022782286629080772, 0.0928780660033226, -0.010215133428573608, -0.05603722855448723, -0.009636969305574894, -0.022743862122297287, 0.0007771056843921542, 0.01370070781558752, -0.021171865984797478, 0.014490406028926373, 0.016907213255763054, -0.025752659887075424, -0.012022086419165134, -0.00025888840900734067, -0.01761372573673725, 0.005479300860315561, -0.04935106262564659, 0.005850387737154961, 0.03852006047964096, 0.03127337992191315]\n"
     ]
    }
   ],
   "source": [
    "text = \"Example sentence to embed\"\n",
    "embedding = embed_model.get_query_embedding(text)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806c58b",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4de10952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b3fc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/joudi/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.34 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   170.20 MiB, ( 1245.92 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2973.75 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    76.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3050.00 MiB, K (f16): 1525.00 MiB, V (f16): 1525.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 627\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.1, # let it be more focused and deterministic, avoid creativity\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5155833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install psycopg2-binary pgvector asyncpg \"sqlalchemy[asyncio]\" greenlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f8bbb",
   "metadata": {},
   "source": [
    "Create the DB and VectorStore where our documents will be stored and retrieved from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d3f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509a4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"1234\"\n",
    "port = \"5432\"\n",
    "user = \"joudi\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96deecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff376ce",
   "metadata": {},
   "source": [
    "We will create a function to process multiple documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907e2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f56953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_document(file_path, vector_store, embed_model):\n",
    "    loader = PyMuPDFReader()\n",
    "    documents = loader.load(file_path)\n",
    "\n",
    "    text_parser = SentenceSplitter(chunk_size=1024)\n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        # Remove null characters from the document text\n",
    "        #clean_text = doc.text.replace(\"\\x00\", \"\")\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(text=text_chunk)\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=\"all\")\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "        nodes.append(node)\n",
    "\n",
    "    vector_store.add(nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c277140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple files\n",
    "file_paths = [\"./data/llama2.pdf\", \"./data/A Scoring System for Assessing Security and Privacy Risks of Pre Installed Applications.pdf\", \n",
    "              \"./data/Android data detection system.pdf\"]\n",
    "file_paths = [\"./data/llama2.pdf\", \"./data/A Scoring System for Assessing Security and Privacy Risks of Pre Installed Applications.pdf\", \n",
    "              \"./data/Android data detection system.pdf\"]\n",
    "for file_path in file_paths:\n",
    "    process_and_store_document(file_path, vector_store, embed_model)\n",
    "    ''',\n",
    "              \"./data/Anonymous Trillemma.pdf\",\n",
    "              \"./data/Examining_the_Integrity_of_Apples_Privacy_Labels_.pdf\",\n",
    "              \"./data/Hidden Permissions on Android_ A Permission Based Android Mobile Privacy Risk Model.pdf\",\n",
    "              '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66fd7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Cui, X., Yu, D., Chan, P., Hui, L. C., Yiu, S. M., & \n",
      "Qing, S. ‘Cochecker: Detecting capability and \n",
      "sensitive data leaks from component chains in \n",
      "android’, Australasian Conference on Information \n",
      "Security and Privacy (2014). 446-453.\n",
      "[7] Chen, H., Leung, H. F., Han, B., & Su, J. ‘Automatic \n",
      "privacy leakage detection for massive android apps via \n",
      "a novel hybrid \n",
      "approach’, IEEE International \n",
      "Conference on Communications (ICC)(2017) 1-7.\n",
      "[8] Cam, N. T., Pham, V. H., & Nguyen, T. ‘Detecting \n",
      "sensitive data leakage via inter-applications on \n",
      "Android using a hybrid analysis technique’, Cluster \n",
      "Computing 22(2019), 1055-1064.\n",
      "[9] Xia, M., Gong, L., Lyu, Y., Qi, Z., & Liu, X, \n",
      "‘Effective real-time android application auditing’, \n",
      "IEEE Symposium on Security and Privacy \n",
      "(2015)899-914.\n",
      "[10] Kul, G., Upadhyaya, S., & Chandola, V, ‘Detecting \n",
      "data leakage from databases on android apps with \n",
      "concept drift’,17th IEEE International Conference On \n",
      "Trust, Security And Privacy In Computing And \n",
      "Communications/12th IEEE International Conference \n",
      "On \n",
      "Big \n",
      "Data \n",
      "Science \n",
      "And \n",
      "Engineering \n",
      "(TrustCom/BigDataSE) (2018) 905-913\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "query_str = \"Can you tell me what are data leaks?\"\n",
    "query_embedding = embed_model.get_query_embedding(query_str)\n",
    "# construct vector store query\n",
    "\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")\n",
    "\n",
    "# returns a VectorStoreQueryResult\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3fe98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e697858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff1f5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "978672a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   20678.29 ms\n",
      "llama_print_timings:      sample time =       2.48 ms /   111 runs   (    0.02 ms per token, 44685.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   80326.79 ms /  1777 tokens (   45.20 ms per token,    22.12 tokens per second)\n",
      "llama_print_timings:        eval time = 1497420.73 ms /   110 runs   (13612.92 ms per token,     0.07 tokens per second)\n",
      "llama_print_timings:       total time = 1577902.41 ms /  1887 tokens\n"
     ]
    }
   ],
   "source": [
    "query_str = \"what are Data leaks\"\n",
    "\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2bd4c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data leaks refer to the unauthorized or unintentional release of sensitive or confidential information. In the context of Android data detection systems, data leaks can occur through various means, such as malicious apps, unsecured APIs, or unintentional errors in code. Data leaks can lead to serious consequences, including privacy violations, financial loss, and reputational damage. Therefore, detecting data leaks is a critical aspect of ensuring the security and privacy of Android devices and applications.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92c2071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Cui, X., Yu, D., Chan, P., Hui, L. C., Yiu, S. M., & \n",
      "Qing, S. ‘Cochecker: Detecting capability and \n",
      "sensitive data leaks from component chains in \n",
      "android’, Australasian Conference on Information \n",
      "Security and Privacy (2014). 446-453.\n",
      "[7] Chen, H., Leung, H. F., Han, B., & Su, J. ‘Automatic \n",
      "privacy leakage detection for massive android apps via \n",
      "a novel hybrid \n",
      "approach’, IEEE International \n",
      "Conference on Communications (ICC)(2017) 1-7.\n",
      "[8] Cam, N. T., Pham, V. H., & Nguyen, T. ‘Detecting \n",
      "sensitive data leakage via inter-applications on \n",
      "Android using a hybrid analysis technique’, Cluster \n",
      "Computing 22(2019), 1055-1064.\n",
      "[9] Xia, M., Gong, L., Lyu, Y., Qi, Z., & Liu, X, \n",
      "‘Effective real-time android application auditing’, \n",
      "IEEE Symposium on Security and Privacy \n",
      "(2015)899-914.\n",
      "[10] Kul, G., Upadhyaya, S., & Chandola, V, ‘Detecting \n",
      "data leakage from databases on android apps with \n",
      "concept drift’,17th IEEE International Conference On \n",
      "Trust, Security And Privacy In Computing And \n",
      "Communications/12th IEEE International Conference \n",
      "On \n",
      "Big \n",
      "Data \n",
      "Science \n",
      "And \n",
      "Engineering \n",
      "(TrustCom/BigDataSE) (2018) 905-913\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a0176",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82967fd4",
   "metadata": {},
   "source": [
    "Try without giving the context to LLM (directly acquiring LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "499e0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d5653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joudi/Desktop/General/RUB/WISE 2425/Programming for Modern Machine Learning/Project/myenv/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'd be happy to help answer your question. A \"data leak\" refers to the unauthorized or unintended disclosure of sensitive or confidential information, which can be a serious threat to individuals, businesses, and organizations. There are several types of data leaks, including: 1. Data breaches: This occurs when an individual's personal information, such as their name, address, social security number, or credit card details, is stolen or accessed without their consent. This can happen due to a variety of reasons, such as a hacked website, a malware attack, or a physical theft of data storage devices. 2. Data exposure: This happens when sensitive information is inadvertently made publicly available due to a software bug, misconfigured database, or poor data handling practices. For example, if a company accidentally posts a list of customer email addresses and passwords online, this could be considered a data exposure. 3. Data theft: This occurs when an individual's personal information is stolen with the intention of using it for malicious purposes, such as identity theft or financial fraud. This can happen through phishing scams, malware attacks\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoConfig, Config\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "    model_file=\"llama-2-13b-chat.Q5_K_M.gguf\",\n",
    "    model_type=\"llama\",\n",
    "    config=AutoConfig(\n",
    "        config=Config(\n",
    "           context_length=4096\n",
    "        )\n",
    "    ),\n",
    "    gpu_layers=50)\n",
    "\n",
    "prompt = \"what are data leaks\"\n",
    "full_prompt = \"A chat between a curious user and an artificial intelligence assistant. \"\\\n",
    "              f\"The assistant gives detailed answers to the user's questions. USER: {prompt} ASSISTANT:\"\n",
    "\n",
    "response = llm(full_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b643fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hello! i'd be happy to help answer your question about data leaks. a data leak refers to the unauthorized release or disclosure of sensitive or confidential information, which can occur due to various reasons such as hacking, human error, or system vulnerabilities. this can lead to serious consequences, including financial loss, reputational damage, legal liability, and even identity theft. there are several types of data leaks, including: 1. unintentional data leakage: this occurs when sensitive information is inadvertently shared or exposed due to human error or system vulnerabilities. for example, an employee may accidentally forward a confidential email to the wrong person, or a software bug may cause sensitive data to be accessible to unauthorized users. 2. intentional data leakage: this occurs when an individual intentionally discloses confidential information, often for personal gain or to harm the organization. for example, an employee may sell confidential information to a competitor or disclose it to the media. 3. data breaches: this refers to the unauthorized access, disclosure, or use of sensitive information due to hacking or other forms of cyber attacks. this\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what are data leaks\"\n",
    "full_prompt = \"A chat between a curious user and an artificial intelligence assistant. \"\\\n",
    "              f\"The assistant gives detailed answers to the user's questions. USER: {prompt} ASSISTANT:\"\n",
    "\n",
    "response = llm(full_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
