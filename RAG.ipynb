{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8630cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.embeddings.bedrock import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01f6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import hashlib\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "from evaluate import load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06eee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neural-bridge/rag-dataset-1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5933a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"/Users/joudi/Measuring_RAG_Effectiveness/data\"\n",
    "\n",
    "CHROMA_PATH = \"/Users/joudi/Measuring_RAG_Effectiveness/database\"  \n",
    "\n",
    "#os.makedirs(CHROMA_PATH)\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9f43b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def add_context_to_chroma(chunks):\\n    db = Chroma(\\n            persist_directory=CHROMA_PATH,\\n            embedding_function=get_embedding_function()\\n        )\\n    #chunks_with_ids = calculate_chunk_ids(chunks)\\n    for chunk in chunks:\\n        chunk.metadata[\"id\"] = generate_unique_id(chunk)  \\n\\n    existing_items = db.get(include=[])\\n    existing_ids = set(existing_items[\"ids\"])\\n    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\\n\\n    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\\n\\n    if len(new_chunks):\\n        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\\n        batch_size = 5000\\n        for i in range(0, len(new_chunks), batch_size):\\n            batch = new_chunks[i:i + batch_size]\\n            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in batch]\\n            db.add_documents(batch, ids=new_chunk_ids)\\n            db.persist()\\n        if new_chunks:\\n            db.add_documents(new_chunks, ids=new_chunk_ids)\\n            db.persist()\\n        print(\"‚úÖ Documents successfully added and persisted.\")\\n    else:\\n        print(\"‚úÖ No new documents to add\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\"\"\"def calculate_chunk_ids(chunks):\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\", \"unknown_source\")\n",
    "        page = chunk.metadata.get(\"page\", \"unknown_page\")\n",
    "        content_hash = hashlib.md5(chunk.page_content.encode(\"utf-8\")).hexdigest()\n",
    "        \n",
    "        # Create a unique ID using source, page, and a hash of the content\n",
    "        chunk_id = f\"{source}:{page}:{content_hash}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\"\"\"\n",
    "\n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "def generate_unique_id(chunk):\n",
    "    # Use current timestamp (in milliseconds) for uniqueness\n",
    "    timestamp = str(int(time.time() * 1000))  # Current timestamp in milliseconds\n",
    "    \n",
    "    # Generate a random number to ensure uniqueness\n",
    "    random_number = random.randint(100000, 999999)\n",
    "\n",
    "    # use document content or metadata to make the ID more unique\n",
    "    content_hash = hashlib.md5(chunk.page_content.encode('utf-8')).hexdigest()[:8]  # First 8 chars of MD5 hash\n",
    "    \n",
    "    # Create a unique ID combining the timestamp and content hash\n",
    "    unique_id = f\"{timestamp}_{content_hash}_{random_number}\"\n",
    "    \n",
    "    return unique_id\n",
    "\n",
    "\n",
    "\"\"\"def add_doc_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function()\n",
    "    )\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "        if new_chunks:\n",
    "            db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        print(\"‚úÖ Documents successfully added and persisted.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\"\"\"\n",
    "\n",
    "def check_num_in_DB(path):\n",
    "    try:\n",
    "        # Load the existing database\n",
    "        db = Chroma(persist_directory=path, embedding_function=get_embedding_function())\n",
    "        existing_items = db.get(include=[])  # Fetch existing document metadata (IDs are always included)\n",
    "        num_existing = len(existing_items[\"ids\"])\n",
    "        print(f\"üìä Number of existing documents in DB: {num_existing}\")\n",
    "        return num_existing\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking existing documents: {e}\")\n",
    "        return 0\n",
    "\n",
    "#for context from DataSet\n",
    "def get_context(ds):\n",
    "    train_chunks = []\n",
    "    test_chunks = []\n",
    "    for context in ds[\"train\"][\"context\"]:\n",
    "        # Convert context string to Document object\n",
    "        document = Document(page_content=context)\n",
    "        # Split the document into smaller chunks\n",
    "        chunked_documents = split_documents([document])\n",
    "        # Add the chunked documents to the train_chunks list\n",
    "        train_chunks.extend(chunked_documents)\n",
    "\n",
    "    # Iterate over the test context data\n",
    "    for context in ds[\"test\"][\"context\"]:\n",
    "        # Convert context string to Document object\n",
    "        document = Document(page_content=context)\n",
    "        # Split the document into smaller chunks\n",
    "        chunked_documents = split_documents([document])\n",
    "        # Add the chunked documents to the test_chunks list\n",
    "        test_chunks.extend(chunked_documents)\n",
    "\n",
    "    return train_chunks, test_chunks\n",
    "\n",
    "def add_context_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function()\n",
    "    )\n",
    "    \n",
    "    # Assign unique IDs to chunks\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"id\"] = generate_unique_id(chunk)\n",
    "    \n",
    "    # Check if the number of new chunks is equal to the number of generated IDs\n",
    "    # this is to extract the IDs we added and check if they already exist\n",
    "    new_chunk_ids = [chunk.metadata[\"id\"] for chunk in chunks]\n",
    "    \n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Number of chunk IDs: {len(new_chunk_ids)}\")\n",
    "    \n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # filter out duplicates to avoid failures :')\n",
    "    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        batch_size = 5000\n",
    "        for i in range(0, len(new_chunks), batch_size):\n",
    "            batch = new_chunks[i:i + batch_size]\n",
    "            batch_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "            print(f\"Adding batch with {len(batch)} documents and {len(batch_ids)} IDs\")\n",
    "            db.add_documents(batch, ids=batch_ids)\n",
    "            db.persist()\n",
    "        print(\"‚úÖ Documents successfully added and persisted.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\n",
    "\n",
    "\"\"\"def add_context_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=get_embedding_function()\n",
    "        )\n",
    "    #chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"id\"] = generate_unique_id(chunk)  \n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        batch_size = 5000\n",
    "        for i in range(0, len(new_chunks), batch_size):\n",
    "            batch = new_chunks[i:i + batch_size]\n",
    "            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "            db.add_documents(batch, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        if new_chunks:\n",
    "            db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        print(\"‚úÖ Documents successfully added and persisted.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919bbeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 1450\n",
      "Number of chunk IDs: 1450\n",
      "Number of existing documents in DB: 12826\n",
      "üëâ Adding new documents: 1450\n",
      "Adding batch with 1450 documents and 1450 IDs\n",
      "‚úÖ Documents successfully added and persisted.\n",
      "üìä Number of existing documents in DB: 14276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tz/_9r1pw_567xd7xk6rb7hb26w0000gn/T/ipykernel_17547/3443059012.py:136: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14276"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#documents = load_documents()\n",
    "#chunks = split_documents(documents)\n",
    "#add_to_chroma(chunks)\n",
    "train_chunk, test_chunk = get_context(ds)\n",
    "#add_context_to_chroma(train_chunk)\n",
    "add_context_to_chroma(test_chunk)\n",
    "check_num_in_DB(CHROMA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab3b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb4af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'answer'],\n",
      "        num_rows: 960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'question', 'answer'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87b87821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The KI Work Environment and Health Award consists of a certificate and 30 000 SEK, to use for further joint activities in the workplace to promote health at work. The prize winner may be an employee/co-worker, a manager, a team, a workplace or a department/equivalent.', \"The ideal spin serve will, if given the chance, bounce twice on the opponent's side of the table, with the second bounce as close to the endline as possible.\", 'The different bat options mentioned are Player Edition, Signature, and Reserve.', \"Some common problems that can damage a roof include the sun's UV rays causing deterioration, snow and ice causing leaks, strong gusts of wind tearing off shingles, overhanging tree limbs rubbing against the roof, birds and small wildlife creating holes, insects chewing at fascia boards and eaves, faulty flashing allowing water to seep in, clogged gutters causing water backup, poorly ventilated attics causing shingles to dry out and become brittle, and lack of roof maintenance leading to significant roof damage.\", 'Gwendolyn Cuff alleged that she was discharged by IBM for the purpose of depriving her of IBM employee benefits in violation of section 510 of the Employee Retirement Income Security Act (\"ERISA\"), 29 U.S.C. ¬ß 1140.', 'The Farmer family got the idea to adopt from China by attending a Steven Curtis Chapman concert.', \"The consequence of a DUI in terms of car insurance according to Nevada laws is that it can lead to the loss of one's license.\", \"In Salem, Massachusetts, visitors can stay at the pet-friendly Salem Inn or the Morning Glory Bed and Breakfast. They can dine on lobster rolls and clam chowder at Lobster Shanty or have crab fritters and pear salad at Rockafellas. Attractions include the Salem Witch Memorial, the grand Custom House, the House of the Seven Gables, and the Peabody Essex Museum. Visitors can also explore the town's architecture, visit the nation‚Äôs oldest confectionery, Ye Olde Pepper Candy Companie, and the pre-1700 Witch House. At midnight, they can take a walk to the common where the witches were hanged.\", 'Sissyboy_D turns on the vacuum cleaner and uses it to pleasure herself.', 'The arrow keys move the ship, x is fire, c is dash, z lets you cycle between available consumables and space activates the chosen consumable.', 'The NCUA has encouraged federally chartered credit unions to offer ‚Äúpayday alternative loans,‚Äù which generally have a longer term than traditional payday products.', 'Premiere Pro is available only on a subscription basis (monthly payments or maybe also yearly payments) whereas Premiere Elements involves a one time purchase of the license.', 'L Glutamine is an essential amino acid that aids in muscle recovery and building muscle. It is also used in the post workout recovery process to manage muscle damage from intense training. When the body is stressed, training at a high level, or undernourished, it may not produce enough L glutamine, and additional L Glutamine may be required to maintain normal levels. It is an energy source that helps build and maintain muscle mass and may inhibit fat storage. By maintaining healthy levels of L Glutamine, cells can maintain optimum hydration, protecting muscles and the immune system from injury during and after exercise.', \"The head coach of Scio High's football team is Kyle Braa and his record is 78-16 overall with six league championships and a pair of 2A state titles.\", \"The main characters in the game Pier Solar and the Great Architects are Hoston, a young botanist, Alina and Edessot. They are on a quest to find a cure for Hoston‚Äôs father's mysterious illness.\", 'Some of the activities mentioned in the context include having sex, masterbating, giving massage, and looking for encounter.', 'Dominguez Sheet Metal Works Inc provides all types of metal fabrications, including stainless steel, copper and aluminum. It also operates an extensive welding shop.', 'The camera has a CCD sensor with 13.8 Megapixels and light sensitivity of 1,600 ISO. It has an LCD screen of size 2.7\" and a resolution of 230k dots. The lens has a zoom of 8x, with a wide angle of 25 mm and a telephoto of 200 mm. The camera weighs 129 g and does not have interchangeable lenses. It supports video format of 720p @ 30fps and has a built-in flash. The storage supports formats like SD, SDXC, and SDHC.', 'The Klean Kanteen water bottle is durable, convenient to use, and affordable. It has a medium-diameter mouth and a sport top for comfortable drinking. It also has a narrower design compared to the standard Nalgene. The bottle performed well in drop tests with minimal damage and ranked second for cold water insulation tests and third for hot water.', 'Some of the new video game releases in the week of November 5, 2017 include Horizon Zero Dawn: The Frozen Wilds, Sonic Forces, Super Lucky‚Äôs Tale, Mario Party: The Top 100, and Need for Speed Payback. Other releases include Doom for Switch, Nioh: Complete Edition, Snipperclips Plus ‚Äì Cut it out, together!, and Wuppo.']\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "n_questions = 20\n",
    "random.seed(11)\n",
    "\n",
    "# get the train answers provided from the dataset\n",
    "for answer in ds[\"train\"][\"answer\"]:\n",
    "    answers.append(answer)\n",
    "\n",
    "random.shuffle(answers)\n",
    "answers = answers[:n_questions]\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e1edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What does the KI Work Environment and Health Award consist of and who can be nominated for it?', 'What is the ideal spin serve in table tennis?', 'What are the different bat options mentioned for the PlayersGrade cricket bat sticker design?', 'What are some common problems that can damage a roof?', \"What was the reason for Gwendolyn Cuff's lawsuit against International Business Machine Company (IBM)?\", 'Who inspired the Farmer family to consider adoption from China?', 'What is the consequence of a DUI in terms of car insurance according to Nevada laws?', 'What are some of the attractions and activities available in Salem, Massachusetts?', 'What does Sissyboy_D do with a vacuum cleaner in the video?', 'What are the controls for the game Stellar Interface?', 'What are some steps the NCUA has taken to encourage federally chartered credit unions?', 'What is the fundamental non-technical difference between Premiere Elements and Premiere Pro?', 'What are the benefits of L Glutamine as described in the product description of Lactonovasport Glucogold?', \"Who is the head coach of Scio High's football team and what is his record?\", 'Who are the main characters in the game Pier Solar and the Great Architects?', 'What are some of the activities mentioned in the context?', 'What are some of the services provided by Dominguez Sheet Metal Works Inc?', 'What are some of the specifications of the camera that the author won from Camera House?', 'What are some of the key features of the Klean Kanteen water bottle?', 'What are some of the new video game releases in the week of November 5, 2017?']\n"
     ]
    }
   ],
   "source": [
    "n_questions = 20\n",
    "random.seed(11)\n",
    "\n",
    "# get the train questions from dataset\n",
    "questions = []\n",
    "for question in ds[\"train\"][\"question\"]:\n",
    "    questions.append(question)\n",
    "\n",
    "    \n",
    "random.shuffle(questions)\n",
    "questions = questions[:n_questions]\n",
    "\n",
    "print(questions)\n",
    "answers_llama = []\n",
    "answers_qwen = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc30ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answers_llama.append({\n",
    "        \"question\": question,\n",
    "        \"answer_rag\": query_rag(question), # check the model used\n",
    "        \"answer_without\": query_without_context(question)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1beecb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answers_qwen.append({\n",
    "        \"question\": question,\n",
    "        \"answer_rag\": query_rag(question), # check the model used\n",
    "        \"answer_without\": query_without_context(question)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eab6af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to answers_qwen.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Save answers_qwen to a CSV file for future review\n",
    "with open('answers_qwen.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer_rag\", \"answer_without\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(answers_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4f34ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_llama.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer_rag\", \"answer_without\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(answers_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "881f21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str):\n",
    "    # get the DB.\n",
    "    embedding_function = get_embedding_function()  \n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    # Build context text from the top results\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    \n",
    "    # Format the prompt using the context and the query\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)  \n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Generate the model's response\n",
    "    #model = Ollama(model=\"llama3.2:latest\")\n",
    "    model = Ollama(model=\"qwen:1.8b\")  \n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    # Get source document IDs\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    \n",
    "    # Format the response for output\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    \n",
    "    # Output the response\n",
    "    #print(formatted_response)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "def query_without_context(query_text: str):\n",
    "    #model = Ollama(model=\"llama3.2:latest\")\n",
    "    model = Ollama(model=\"qwen:1.8b\") \n",
    "\n",
    "    # Generate the response from the model, without sending any context\n",
    "    response_text = model.invoke(query_text)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08398b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using metrics for Evaluation\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8ed5665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Qwen model with context is: [0.8285099267959595, 0.8499962687492371, 0.7705177068710327, 0.845622181892395, 0.8056120276451111, 0.7852283716201782, 0.7942086458206177, 0.8307557106018066, 0.8317534923553467, 0.806027889251709, 0.8206638097763062, 0.7793591022491455, 0.8649715185165405, 0.9355557560920715, 0.8173287510871887, 0.829719603061676, 0.7926945686340332, 0.7574712038040161, 0.7954857349395752, 0.8515809178352356]\n",
      "Average precision of Qwen model with context is: 0.820\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the model with RAG\n",
    "predictions_qwen_rag = [entry[\"answer_rag\"] for entry in answers_qwen]\n",
    "results_qwen = bertscore.compute(predictions=predictions_qwen_rag, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Qwen model with context is: {results_qwen['precision']}\")\n",
    "print(f\"Average precision of Qwen model with context is: {sum(results_qwen['precision']) / len(results_qwen['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02b9c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Llama model with context is: [0.8579671382904053, 0.7815169095993042, 0.8120019435882568, 0.7957863807678223, 0.884261965751648, 0.8656924962997437, 0.8263412714004517, 0.7797498106956482, 0.8589804768562317, 0.8272422552108765, 0.8086303472518921, 0.7652955055236816, 0.837438702583313, 0.8595802187919617, 0.7835190892219543, 0.8305968642234802, 0.8292722702026367, 0.7986270189285278, 0.8563112020492554, 0.853237509727478]\n",
      "Average precision of Llama model with context is: 0.826\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the model Llama with RAG\n",
    "predictions_llama_rag = [entry[\"answer_rag\"] for entry in answers_llama]\n",
    "results_llama = bertscore.compute(predictions=predictions_llama_rag, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Llama model with context is: {results_llama['precision']}\")\n",
    "print(f\"Average precision of Llama model with context is: {sum(results_llama['precision']) / len(results_llama['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd5d14b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Qwen model without context is: [0.825782060623169, 0.8093596696853638, 0.8117802143096924, 0.8431107997894287, 0.8258489370346069, 0.8479394912719727, 0.8436204791069031, 0.832645058631897, 0.8574590086936951, 0.8032870888710022, 0.8134545683860779, 0.8138843774795532, 0.8410120010375977, 0.8866012692451477, 0.8397082686424255, 0.8159328699111938, 0.853897750377655, 0.8333093523979187, 0.8527597784996033, 0.8264411091804504]\n",
      "Average precision of Qwen model without context is: 0.834\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the Qwen _without_ RAG\n",
    "predictions_qwen = [entry[\"answer_without\"] for entry in answers_qwen]\n",
    "results_qwen = bertscore.compute(predictions=predictions_qwen, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Qwen model without context is: {results_qwen['precision']}\")\n",
    "print(f\"Average precision of Qwen model without context is: {sum(results_qwen['precision']) / len(results_qwen['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05036316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Llama model without context is: [0.8343126177787781, 0.8416059613227844, 0.7805914282798767, 0.8608595728874207, 0.7548799514770508, 0.8405669927597046, 0.7790666818618774, 0.8278319239616394, 0.8153802752494812, 0.7961475253105164, 0.8211145401000977, 0.7652231454849243, 0.8434082269668579, 0.8717678785324097, 0.803914487361908, 0.8297935128211975, 0.8022599220275879, 0.8169444799423218, 0.8502309918403625, 0.7981246709823608]\n",
      "Average precision of Llama model without context is: 0.817\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the Llama _without_ RAG\n",
    "predictions_llama = [entry[\"answer_without\"] for entry in answers_llama]\n",
    "results_llama = bertscore.compute(predictions=predictions_llama, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Llama model without context is: {results_llama['precision']}\")\n",
    "print(f\"Average precision of Llama model without context is: {sum(results_llama['precision']) / len(results_llama['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8943c0",
   "metadata": {},
   "source": [
    "-------------\n",
    "On the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab12f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What was the business model of the Donald Trump Network?', \"What role did the author's mother play in the lives of the two girls mentioned in the context?\", 'What are some potential solutions for the Outlook Send Error?', 'Who is the mayor of Clinton seeking his eighth term?', 'Who was Jessie Foster living with in Las Vegas before her disappearance?', \"What are some of Jesse Oliver's musical influences?\", 'What was the difference in views between Public Protector Busisiwe Mkhwebane and her predecessor Thuli Madonsela on the terms of reference of the judicial commission of inquiry into state capture?', 'What are the pros and cons of playing Katarina in a game?', \"What is the woman's preference in the context?\", 'What are some of the uses for the mobile homes produced?', 'What is the function of the BK-channel blocker GAL021 in relation to opioid-induced respiratory depression?', 'Where did the culture of drag racing originate?', 'Who were the two top executives at Vice Media that were put on leave amid a sexual harassment scandal?', 'What types of fruit does the welfare orchard run by the Church grow?', 'What is the setting and theme of the game Kingdom Come: Deliverance?', 'Who is the Chairman of the Civil Committee of the District Court?', 'What determines the spiciness of ginger candies?', 'What is the purpose of the Gara Pagos series in mobile crusher?', 'What is the purpose of a storyboard in animated video production?', 'What is the active component in garcinia cambogia extract and what is its potential effect on weight loss?']\n"
     ]
    }
   ],
   "source": [
    "n_questions = 20\n",
    "random.seed(11)\n",
    "\n",
    "# get the questions from Test dataset\n",
    "questions_test = []\n",
    "for question in ds[\"test\"][\"question\"]:\n",
    "    questions_test.append(question)\n",
    "\n",
    "    \n",
    "random.shuffle(questions_test)\n",
    "questions_test = questions_test[:n_questions]\n",
    "\n",
    "print(questions_test)\n",
    "answers_llama_test = []\n",
    "answers_qwen_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ddc9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions_test:\n",
    "    answers_llama_test.append({\n",
    "        \"question\": question,\n",
    "        \"answer_rag\": query_rag(question), # check the model used\n",
    "        \"answer_without\": query_without_context(question)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69167937",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions_test:\n",
    "    answers_qwen_test.append({\n",
    "        \"question\": question,\n",
    "        \"answer_rag\": query_rag(question), # check the model used\n",
    "        \"answer_without\": query_without_context(question)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cebbf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Donald Trump Network sold both the network‚Äôs ‚Äòname‚Äô and supplements. There was compensation for referring new people to the network and also for selling supplements. New clients would get a urine test kit to collect a sample of their urine, which would be sent to a laboratory for analysis. Various supplements in different combinations were then prescribed and would be sent to the client or members of the network each month.', \"The author's mother was the Guardian ad Litem for the two girls, assigned by the court system to look out for their best interests. She petitioned for their needs, gave opinions on their living arrangements, and established a positive and stable relationship with them.\", \"Some potential solutions for the Outlook Send Error include creating a new Outlook profile and re-adding your data files, removing your Outlook data files from your Account Settings -> Data files list and adding them back again, and disabling your anti-virus for the operation. If the error is due to a corrupt profile, the problematic profile needs to be removed. If Outlook is connecting to a POP3 server and the user changed his password without updating Outlook, the password needs to be updated in Outlook. If there's a corrupt message in the user's mailbox in a POP3 environment, it needs to be dealt with. If you're sending or receiving a large email attachment, your email account remains locked.\", 'Lew Starling', 'Jessie Foster was living with 39-year-old Peter Todd, who was well-known to police as a pimp.', \"Jesse Oliver's musical influences include Sisters of Mercy, Fields of the Nephilim, Depeche Mode, Massive Attack, Interpol and The Cure. He also listens to a lot of electronic music, including Rotterdam Hardcore and Goa Trance.\", 'Thuli Madonsela believed that the terms of reference should be based on what she had investigated and there was no room to expand the commission to include what was never investigated. On the other hand, Busisiwe Mkhwebane called for a wider probe into state capture, urging that the terms of reference should be broad enough to include the capture of all state institutions and state-owned entities.', 'The pros of playing Katarina include fast minion wave clearing, strong late game, no mana usage, great teamfighting, and high rewards. The cons include the need for early game kills to be effective, inability to tank like Malphite, being a melee mage, and getting focused a lot.', 'The woman prefers someone honest and sincere, who she can possibly have a relationship with. She is not interested in freaks and prefers someone who is HWP, DDF and SWM.', 'The houses can be used as summer lake or sea houses, residential, offices, social housing, commercial buildings or warehouses.', 'The BK-channel blocker GAL021 is a respiratory stimulant that can reverse opioid-induced respiratory depression. It acts at K(+) -channels expressed on type 1 carotid body cells and does not interact with the opioid system, therefore it does not compromise opioid analgesic efficacy.', 'The culture of drag racing started on the dry lake beds of California.', 'The two top executives at Vice Media that were put on leave amid a sexual harassment scandal were company president Andrew Creighton and chief digital officer Mike Germano.', 'The welfare orchard run by the Church grows peaches, apricots, apples, cherries, almonds, etc.', 'The game Kingdom Come: Deliverance is set within the Holy Roman Empire during the late Middle Ages. It promises a first-person seat to a harsh and brutal power struggle for the throne. The game draws its inspiration from historically authentic characters, themes, and warfare, with no magic, high fantasy or mythical overtones.', 'Judge Gibson is the Chairman of the Civil Committee of the District Court.', 'The spiciness of the candy is dependent on the age of the ginger, the older the ginger is, the more spicy the candies will turn out.', 'A building is demolished and natural stones dug out during civil engineering work with mobile crusher, and marketed Gara Pagos series for this purpose.', 'The purpose of a storyboard in animated video production is to provide a basic visual sense of how the whole video will be put together. It helps the production team understand every scene before production begins. It is also used to write the dialogue under each frame so everyone can see what‚Äôs being said for each scene.', 'The active component in garcinia cambogia extract is hydroxycitric acid (HCA). It appears to inhibit the enzyme ATP-citrate lyase, diverting carbohydrates away from fat storage and towards stored energy as glycogen. Increased glycogen levels are thought to send a signal to the brain that the body is ‚Äúfull,‚Äù causing reduced appetite and food intake. It also seems to stimulate serotonin release which is associated with decreased food intake. Additionally, it may help break down the protective perilipin layer surrounding fat cells, making it easier for the body to burn fat cells.']\n"
     ]
    }
   ],
   "source": [
    "answers_test = []\n",
    "n_questions = 20\n",
    "random.seed(11)\n",
    "\n",
    "# get the answers provided from the dataset\n",
    "for answer in ds[\"test\"][\"answer\"]:\n",
    "    answers_test.append(answer)\n",
    "\n",
    "random.shuffle(answers_test)\n",
    "answers_test = answers_test[:n_questions]\n",
    "print(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dc332cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Qwen model with context using test DS is: [0.7707095742225647, 0.8602573275566101, 0.8458157777786255, 0.8468570113182068, 0.8512611985206604, 0.8327263593673706, 0.7890494465827942, 0.8119370937347412, 0.8579158782958984, 0.7438424229621887, 0.89805668592453, 0.8437221050262451, 0.9425426721572876, 0.8266360759735107, 0.8330429792404175, 0.8862369656562805, 0.8400332927703857, 0.8153977394104004, 0.8804709911346436, 0.8650380373001099]\n",
      "Average precision of Qwen model with context using test DS is: 0.842\n"
     ]
    }
   ],
   "source": [
    "# compare the Test DS answers of the model with RAG\n",
    "predictions_qwen_rag_test = [entry[\"answer_rag\"] for entry in answers_qwen_test]\n",
    "results_qwen_test = bertscore.compute(predictions=predictions_qwen_rag_test, references=answers_test, lang=\"en\")\n",
    "print(f\"Precision of Qwen model with context using test DS is: {results_qwen_test['precision']}\")\n",
    "print(f\"Average precision of Qwen model with context using test DS is: {sum(results_qwen_test['precision']) / len(results_qwen_test['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbc0c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Llama model with context is: [0.8896971940994263, 0.9132505059242249, 0.8579166531562805, 0.9927899837493896, 0.9439313411712646, 0.8285902738571167, 0.8887848854064941, 0.8703585267066956, 0.8441950678825378, 0.8317444920539856, 0.9285246133804321, 0.8445356488227844, 0.9482240676879883, 0.8733073472976685, 0.8327681422233582, 0.8435680270195007, 0.9697893261909485, 0.8459093570709229, 0.8999610543251038, 0.9609237313270569]\n",
      "Average precision of Llama model with context is: 0.890\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the model Llama with RAG\n",
    "predictions_llama_rag_test = [entry[\"answer_rag\"] for entry in answers_llama_test]\n",
    "results_llama_test = bertscore.compute(predictions=predictions_llama_rag_test, references=answers_test, lang=\"en\")\n",
    "print(f\"Precision of Llama model with context is: {results_llama_test['precision']}\")\n",
    "print(f\"Average precision of Llama model with context is: {sum(results_llama_test['precision']) / len(results_llama_test['precision']):0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python RAG",
   "language": "python",
   "name": "measuring_rag_effectiveness"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
