{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8630cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.embeddings.bedrock import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01f6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import hashlib\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "from evaluate import load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06eee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"neural-bridge/rag-dataset-1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5933a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"/Users/joudi/Measuring_RAG_Effectiveness/data\"\n",
    "\n",
    "CHROMA_PATH = \"/Users/joudi/Measuring_RAG_Effectiveness/database\"  \n",
    "\n",
    "#os.makedirs(CHROMA_PATH)\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9f43b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def add_context_to_chroma(chunks):\\n    db = Chroma(\\n            persist_directory=CHROMA_PATH,\\n            embedding_function=get_embedding_function()\\n        )\\n    #chunks_with_ids = calculate_chunk_ids(chunks)\\n    for chunk in chunks:\\n        chunk.metadata[\"id\"] = generate_unique_id(chunk)  \\n\\n    existing_items = db.get(include=[])\\n    existing_ids = set(existing_items[\"ids\"])\\n    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\\n\\n    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\\n\\n    if len(new_chunks):\\n        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\\n        batch_size = 5000\\n        for i in range(0, len(new_chunks), batch_size):\\n            batch = new_chunks[i:i + batch_size]\\n            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in batch]\\n            db.add_documents(batch, ids=new_chunk_ids)\\n            db.persist()\\n        if new_chunks:\\n            db.add_documents(new_chunks, ids=new_chunk_ids)\\n            db.persist()\\n        print(\"‚úÖ Documents successfully added and persisted.\")\\n    else:\\n        print(\"‚úÖ No new documents to add\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\", \"unknown_source\")\n",
    "        page = chunk.metadata.get(\"page\", \"unknown_page\")\n",
    "        content_hash = hashlib.md5(chunk.page_content.encode(\"utf-8\")).hexdigest()\n",
    "        \n",
    "        # Create a unique ID using source, page, and a hash of the content\n",
    "        chunk_id = f\"{source}:{page}:{content_hash}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "def generate_unique_id(chunk):\n",
    "    # Use current timestamp (in milliseconds) for uniqueness\n",
    "    timestamp = str(int(time.time() * 1000))  # Current timestamp in milliseconds\n",
    "    \n",
    "    # Generate a random number to ensure uniqueness\n",
    "    random_number = random.randint(100000, 999999)\n",
    "\n",
    "    # use document content or metadata to make the ID more unique\n",
    "    content_hash = hashlib.md5(chunk.page_content.encode('utf-8')).hexdigest()[:8]  # First 8 chars of MD5 hash\n",
    "    \n",
    "    # Create a unique ID combining the timestamp and content hash\n",
    "    unique_id = f\"{timestamp}_{content_hash}_{random_number}\"\n",
    "    \n",
    "    return unique_id\n",
    "\n",
    "\n",
    "def add_doc_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function()\n",
    "    )\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "        if new_chunks:\n",
    "            db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        print(\"‚úÖ Documents successfully added and persisted.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\n",
    "\n",
    "def check_num_in_DB(path):\n",
    "    try:\n",
    "        # Load the existing database\n",
    "        db = Chroma(persist_directory=path, embedding_function=get_embedding_function())\n",
    "        existing_items = db.get(include=[])  # Fetch existing document metadata (IDs are always included)\n",
    "        num_existing = len(existing_items[\"ids\"])\n",
    "        print(f\"üìä Number of existing documents in DB: {num_existing}\")\n",
    "        return num_existing\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking existing documents: {e}\")\n",
    "        return 0\n",
    "\n",
    "#for context from DataSet\n",
    "def get_context(ds):\n",
    "    train_chunks = []\n",
    "    test_chunks = []\n",
    "    for context in ds[\"train\"][\"context\"]:\n",
    "        # Convert context string to Document object\n",
    "        document = Document(page_content=context)\n",
    "        # Split the document into smaller chunks\n",
    "        chunked_documents = split_documents([document])\n",
    "        # Add the chunked documents to the train_chunks list\n",
    "        train_chunks.extend(chunked_documents)\n",
    "\n",
    "    # Iterate over the test context data\n",
    "    for context in ds[\"test\"][\"context\"]:\n",
    "        # Convert context string to Document object\n",
    "        document = Document(page_content=context)\n",
    "        # Split the document into smaller chunks\n",
    "        chunked_documents = split_documents([document])\n",
    "        # Add the chunked documents to the test_chunks list\n",
    "        test_chunks.extend(chunked_documents)\n",
    "\n",
    "    return train_chunks, test_chunks\n",
    "\n",
    "def add_context_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function()\n",
    "    )\n",
    "    \n",
    "    # Assign unique IDs to chunks\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"id\"] = generate_unique_id(chunk)\n",
    "    \n",
    "    # Check if the number of new chunks is equal to the number of generated IDs\n",
    "    new_chunk_ids = [chunk.metadata[\"id\"] for chunk in chunks]\n",
    "    \n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Number of chunk IDs: {len(new_chunk_ids)}\")\n",
    "    \n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        batch_size = 5000\n",
    "        for i in range(0, len(new_chunks), batch_size):\n",
    "            batch = new_chunks[i:i + batch_size]\n",
    "            batch_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "            print(f\"Adding batch with {len(batch)} documents and {len(batch_ids)} IDs\")\n",
    "            db.add_documents(batch, ids=batch_ids)\n",
    "            db.persist()\n",
    "        print(\"‚úÖ Documents successfully added and persisted.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\n",
    "\n",
    "\"\"\"def add_context_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=get_embedding_function()\n",
    "        )\n",
    "    #chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"id\"] = generate_unique_id(chunk)  \n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"üëâ Adding new documents: {len(new_chunks)}\")\n",
    "        batch_size = 5000\n",
    "        for i in range(0, len(new_chunks), batch_size):\n",
    "            batch = new_chunks[i:i + batch_size]\n",
    "            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "            db.add_documents(batch, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        if new_chunks:\n",
    "            db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        print(\"‚úÖ Documents successfully added and persisted.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new documents to add\")\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919bbeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tz/_9r1pw_567xd7xk6rb7hb26w0000gn/T/ipykernel_12674/36839387.py:8: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
      "/var/folders/tz/_9r1pw_567xd7xk6rb7hb26w0000gn/T/ipykernel_12674/3443059012.py:74: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=path, embedding_function=get_embedding_function())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Number of existing documents in DB: 12826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12826"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#documents = load_documents()\n",
    "#chunks = split_documents(documents)\n",
    "#add_to_chroma(chunks)\n",
    "#train_chunk, test_chunk = get_context(ds)\n",
    "#add_context_to_chroma(train_chunk)\n",
    "#add_context_to_chroma(test_chunk)\n",
    "check_num_in_DB(CHROMA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab3b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5eb4af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'answer'],\n",
      "        num_rows: 960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'question', 'answer'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "87b87821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The KI Work Environment and Health Award consists of a certificate and 30 000 SEK, to use for further joint activities in the workplace to promote health at work. The prize winner may be an employee/co-worker, a manager, a team, a workplace or a department/equivalent.', \"The ideal spin serve will, if given the chance, bounce twice on the opponent's side of the table, with the second bounce as close to the endline as possible.\", 'The different bat options mentioned are Player Edition, Signature, and Reserve.', \"Some common problems that can damage a roof include the sun's UV rays causing deterioration, snow and ice causing leaks, strong gusts of wind tearing off shingles, overhanging tree limbs rubbing against the roof, birds and small wildlife creating holes, insects chewing at fascia boards and eaves, faulty flashing allowing water to seep in, clogged gutters causing water backup, poorly ventilated attics causing shingles to dry out and become brittle, and lack of roof maintenance leading to significant roof damage.\", 'Gwendolyn Cuff alleged that she was discharged by IBM for the purpose of depriving her of IBM employee benefits in violation of section 510 of the Employee Retirement Income Security Act (\"ERISA\"), 29 U.S.C. ¬ß 1140.', 'The Farmer family got the idea to adopt from China by attending a Steven Curtis Chapman concert.', \"The consequence of a DUI in terms of car insurance according to Nevada laws is that it can lead to the loss of one's license.\", \"In Salem, Massachusetts, visitors can stay at the pet-friendly Salem Inn or the Morning Glory Bed and Breakfast. They can dine on lobster rolls and clam chowder at Lobster Shanty or have crab fritters and pear salad at Rockafellas. Attractions include the Salem Witch Memorial, the grand Custom House, the House of the Seven Gables, and the Peabody Essex Museum. Visitors can also explore the town's architecture, visit the nation‚Äôs oldest confectionery, Ye Olde Pepper Candy Companie, and the pre-1700 Witch House. At midnight, they can take a walk to the common where the witches were hanged.\", 'Sissyboy_D turns on the vacuum cleaner and uses it to pleasure herself.', 'The arrow keys move the ship, x is fire, c is dash, z lets you cycle between available consumables and space activates the chosen consumable.', 'The NCUA has encouraged federally chartered credit unions to offer ‚Äúpayday alternative loans,‚Äù which generally have a longer term than traditional payday products.', 'Premiere Pro is available only on a subscription basis (monthly payments or maybe also yearly payments) whereas Premiere Elements involves a one time purchase of the license.', 'L Glutamine is an essential amino acid that aids in muscle recovery and building muscle. It is also used in the post workout recovery process to manage muscle damage from intense training. When the body is stressed, training at a high level, or undernourished, it may not produce enough L glutamine, and additional L Glutamine may be required to maintain normal levels. It is an energy source that helps build and maintain muscle mass and may inhibit fat storage. By maintaining healthy levels of L Glutamine, cells can maintain optimum hydration, protecting muscles and the immune system from injury during and after exercise.', \"The head coach of Scio High's football team is Kyle Braa and his record is 78-16 overall with six league championships and a pair of 2A state titles.\", \"The main characters in the game Pier Solar and the Great Architects are Hoston, a young botanist, Alina and Edessot. They are on a quest to find a cure for Hoston‚Äôs father's mysterious illness.\", 'Some of the activities mentioned in the context include having sex, masterbating, giving massage, and looking for encounter.', 'Dominguez Sheet Metal Works Inc provides all types of metal fabrications, including stainless steel, copper and aluminum. It also operates an extensive welding shop.', 'The camera has a CCD sensor with 13.8 Megapixels and light sensitivity of 1,600 ISO. It has an LCD screen of size 2.7\" and a resolution of 230k dots. The lens has a zoom of 8x, with a wide angle of 25 mm and a telephoto of 200 mm. The camera weighs 129 g and does not have interchangeable lenses. It supports video format of 720p @ 30fps and has a built-in flash. The storage supports formats like SD, SDXC, and SDHC.', 'The Klean Kanteen water bottle is durable, convenient to use, and affordable. It has a medium-diameter mouth and a sport top for comfortable drinking. It also has a narrower design compared to the standard Nalgene. The bottle performed well in drop tests with minimal damage and ranked second for cold water insulation tests and third for hot water.', 'Some of the new video game releases in the week of November 5, 2017 include Horizon Zero Dawn: The Frozen Wilds, Sonic Forces, Super Lucky‚Äôs Tale, Mario Party: The Top 100, and Need for Speed Payback. Other releases include Doom for Switch, Nioh: Complete Edition, Snipperclips Plus ‚Äì Cut it out, together!, and Wuppo.']\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "n_questions = 20\n",
    "random.seed(11)\n",
    "\n",
    "# get the answers provided from the dataset\n",
    "for answer in ds[\"train\"][\"answer\"]:\n",
    "    answers.append(answer)\n",
    "\n",
    "random.shuffle(answers)\n",
    "answers = answers[:n_questions]\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "47e1edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What does the KI Work Environment and Health Award consist of and who can be nominated for it?', 'What is the ideal spin serve in table tennis?', 'What are the different bat options mentioned for the PlayersGrade cricket bat sticker design?', 'What are some common problems that can damage a roof?', \"What was the reason for Gwendolyn Cuff's lawsuit against International Business Machine Company (IBM)?\", 'Who inspired the Farmer family to consider adoption from China?', 'What is the consequence of a DUI in terms of car insurance according to Nevada laws?', 'What are some of the attractions and activities available in Salem, Massachusetts?', 'What does Sissyboy_D do with a vacuum cleaner in the video?', 'What are the controls for the game Stellar Interface?', 'What are some steps the NCUA has taken to encourage federally chartered credit unions?', 'What is the fundamental non-technical difference between Premiere Elements and Premiere Pro?', 'What are the benefits of L Glutamine as described in the product description of Lactonovasport Glucogold?', \"Who is the head coach of Scio High's football team and what is his record?\", 'Who are the main characters in the game Pier Solar and the Great Architects?', 'What are some of the activities mentioned in the context?', 'What are some of the services provided by Dominguez Sheet Metal Works Inc?', 'What are some of the specifications of the camera that the author won from Camera House?', 'What are some of the key features of the Klean Kanteen water bottle?', 'What are some of the new video game releases in the week of November 5, 2017?']\n"
     ]
    }
   ],
   "source": [
    "n_questions = 20\n",
    "random.seed(11)\n",
    "\n",
    "# get the questions from dataset\n",
    "questions = []\n",
    "for question in ds[\"train\"][\"question\"]:\n",
    "    questions.append(question)\n",
    "\n",
    "    \n",
    "random.shuffle(questions)\n",
    "questions = questions[:n_questions]\n",
    "\n",
    "print(questions)\n",
    "answers_llama = []\n",
    "answers_qwen = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc30ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answers_llama.append({\n",
    "        \"question\": question,\n",
    "        \"answer_rag\": query_rag(question), # check the model used\n",
    "        \"answer_without\": query_without_context(question)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1beecb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answers_qwen.append({\n",
    "        \"question\": question,\n",
    "        \"answer_rag\": query_rag(question), # check the model used\n",
    "        \"answer_without\": query_without_context(question)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eab6af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to answers_qwen.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Save answers_qwen to a CSV file for future review\n",
    "with open('answers_qwen.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer_rag\", \"answer_without\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(answers_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4f34ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_llama.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"question\", \"answer_rag\", \"answer_without\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(answers_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "881f21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str):\n",
    "    # get the DB.\n",
    "    embedding_function = get_embedding_function()  \n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    # Build context text from the top results\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    \n",
    "    # Format the prompt using the context and the query\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)  \n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Generate the model's response\n",
    "    model = Ollama(model=\"llama3.2:latest\")\n",
    "    #model = Ollama(model=\"qwen:1.8b\")  \n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    # Get source document IDs\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    \n",
    "    # Format the response for output\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    \n",
    "    # Output the response\n",
    "    #print(formatted_response)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "def query_without_context(query_text: str):\n",
    "    model = Ollama(model=\"llama3.2:latest\")\n",
    "    #model = Ollama(model=\"qwen:1.8b\") \n",
    "\n",
    "    # Generate the response from the model, without sending any context\n",
    "    response_text = model.invoke(query_text)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08398b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae7b1bfc4134773bf725e3bb812a117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b0c46b4e9c4c1bad66a4f991ec7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ba8b5ed71d46a9a1f8c97a3688810a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e25e8d366346dabb8f315c7ccd4c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7cbe2de45b44e4a76e308f46917d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e814ab8be648c09218c744c73c210d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# using metrics for Evaluation\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b8ed5665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Qwen model with context is: [0.9065226316452026, 0.9070388078689575, 0.8869307041168213, 0.8989588022232056, 0.9240614771842957, 0.8878748416900635, 0.8651293516159058, 0.8624709844589233, 0.8369998931884766, 0.8711046576499939, 0.8568368554115295, 0.9193529486656189, 0.8791593313217163, 0.9931979775428772, 0.8557924032211304, 0.8376179337501526, 0.877596378326416, 0.8185430765151978, 0.8468308448791504, 0.9110895395278931]\n",
      "Average precision of Qwen model with context is: 0.882\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the model with RAG\n",
    "predictions_qwen_rag = [entry[\"answer_rag\"] for entry in answers_qwen]\n",
    "results_qwen = bertscore.compute(predictions=predictions_qwen_rag, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Qwen model with context is: {results_qwen['precision']}\")\n",
    "print(f\"Average precision of Qwen model with context is: {sum(results_qwen['precision']) / len(results_qwen['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Llama model with context is: [0.9065226316452026, 0.9070388078689575, 0.8869307041168213, 0.8989588022232056, 0.9240614771842957, 0.8878748416900635, 0.8651293516159058, 0.8624709844589233, 0.8369998931884766, 0.8711046576499939, 0.8568368554115295, 0.9193529486656189, 0.8791593313217163, 0.9931979775428772, 0.8557924032211304, 0.8376179337501526, 0.877596378326416, 0.8185430765151978, 0.8468308448791504, 0.9110895395278931]\n",
      "Average precision of Llama model with context is: 0.881\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the model with RAG\n",
    "predictions_llama_rag = [entry[\"answer_rag\"] for entry in answers_llama]\n",
    "results_llama = bertscore.compute(predictions=predictions_llama_rag, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Llama model with context is: {results_qwen['precision']}\")\n",
    "print(f\"Average precision of Llama model with context is: {sum(results_llama['precision']) / len(results_llama['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bd5d14b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Qwen model without context is: [0.8662627935409546, 0.7969239950180054, 0.812042236328125, 0.8282989859580994, 0.8162297010421753, 0.8394553661346436, 0.7863755226135254, 0.8133270740509033, 0.847653865814209, 0.8154497742652893, 0.8009121417999268, 0.7930828332901001, 0.8174949884414673, 0.8692981004714966, 0.8750858306884766, 0.8297284245491028, 0.8049622774124146, 0.8117737174034119, 0.8218947052955627, 0.8374470472335815]\n",
      "Average precision of Qwen model without context is: 0.824\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the Qwen _without_ RAG\n",
    "predictions_qwen = [entry[\"answer_without\"] for entry in answers_qwen]\n",
    "results_qwen = bertscore.compute(predictions=predictions_qwen, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Qwen model without context is: {results_qwen['precision']}\")\n",
    "print(f\"Average precision of Qwen model without context is: {sum(results_qwen['precision']) / len(results_qwen['precision']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "05036316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Llama model without context is: [0.8662627935409546, 0.7969239950180054, 0.812042236328125, 0.8282989859580994, 0.8162297010421753, 0.8394553661346436, 0.7863755226135254, 0.8133270740509033, 0.847653865814209, 0.8154497742652893, 0.8009121417999268, 0.7930828332901001, 0.8174949884414673, 0.8692981004714966, 0.8750858306884766, 0.8297284245491028, 0.8049622774124146, 0.8117737174034119, 0.8218947052955627, 0.8374470472335815]\n",
      "Average precision of Llama model without context is: 0.825\n"
     ]
    }
   ],
   "source": [
    "# compare the answers of the Llama _without_ RAG\n",
    "predictions_llama = [entry[\"answer_without\"] for entry in answers_llama]\n",
    "results_llama = bertscore.compute(predictions=predictions_llama, references=answers, lang=\"en\")\n",
    "print(f\"Precision of Llama model without context is: {results_qwen['precision']}\")\n",
    "print(f\"Average precision of Llama model without context is: {sum(results_llama['precision']) / len(results_llama['precision']):0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python rag",
   "language": "python",
   "name": "measuring_effectiveness_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
